{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TTLT_Cassava.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6f4ngvqfoEU"
      },
      "source": [
        "# A Primer on Artificial Intelligence in Plant Digital Phenomics: Embarking on the Data to Insights Journey (*Tutorial*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JgHbN1afywA"
      },
      "source": [
        "This tutorial is a supplement to the paper **A Primer on Artificial Intelligence in Plant Digital Phenomics: Embarking on the Data to Insights Journey** (submitted to *Trends in Plant Science, 2021*) by Antoine L. Harfouche, Farid Nakhle, Orlando G. Sardella, Antoine H.\n",
        "Harfouche, Eli Dart, and Daniel Jacobson.\n",
        "\n",
        "Read the accompanying paper [here](https://doi.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjn6pRhha64"
      },
      "source": [
        "This interactive tutorial aims to train, for the first time, an interpretable by design model to identify and classify cassava plant diseases, and to explain its predictions.\n",
        "\n",
        "This tutorial covers:\n",
        "- Downloading and extracting a dataset to Google Colab from a remote repository.\n",
        "- Exploring the dataset class distribution frequency using descriptive data analysis.\n",
        "- Training the interpretable by design 'this looks like that' explainable artificial intelligence (X-AI) algorithm with an augmented training dataset.\n",
        "- Analyzing the model performance by generating a confusion matrix using a test dataset.\n",
        "- Generating explanations for the predictions made by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjHnwEdxjbc9"
      },
      "source": [
        "**NB:** \n",
        "- Basic data preprocessing steps, including data splitting, balancing, cropping, and segmenting are explained in our previous tutorial **Ready, Steady, Go AI: A Practical Tutorial on Fundamentals of Artificial Intelligence and Its Applications in Phenomics Image Analysis** where their code is implemented in interactive notebooks hosted on our Github repository at https://github.com/HarfoucheLab/Ready-Steady-Go-AI. These steps will be reffered to in this tutorial where needed, directly linking to the corresponding notebook.\n",
        "- 'This looks like that' algorithm was created, introduced, and developed by Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. 2019. This looks like that: deep learning for interpretable image recognition. Proceedings of the 33rd International Conference on Neural Information Processing Systems. Curran Associates Inc., Red Hook, NY, USA, Article 801, 8930â€“8941.\n",
        "- The cassava dataset consists of 21,397 labeled images collected during a regular survey in Uganda where images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala. The dataset is publicly available on the Kaggle repository at https://www.kaggle.com/c/cassava-leaf-disease-classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkAvHgLblci5"
      },
      "source": [
        "Before diving into the code to train the interpretable by design 'this looks like that' X-AI algorithm, the next section will briefly cover the differences between post-hoc explainable models and interpretable by design models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrZWea8Yeq5g"
      },
      "source": [
        "#Opening the Black Box *vs.* Designing a Transparent Glass Box"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAbBT1IPj0xN"
      },
      "source": [
        "AI models are commonly referred to as a black boxes because they do not reveal their internal mechanisms to their users. Such models are created directly from data and, not even the scientists who created them can understand or explain what exactly is happening inside them or how they made a specific prediction.\n",
        "As AI becomes more advanced and widely adopted, scientists are challenged to comprehend and retrace how a model came to a prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqw_S9FipSxQ"
      },
      "source": [
        "In an attempt towards opening black box models, approaches that make the inner workings of AI models understandable to humans have been developed. These approaches consist of creating a second (post-hoc) model to explain the first black box model. Post-hoc models can be classified based on whether they are applicable to all AI algorithms (i.e., model-agnostic) or only to one AI algorithm (i.e., model-specific); they often employ data perturbation strategies which involve modifying the input data and observing the changes in the black box model predictions. Based on these changes, they identify which parts of data have been important for the predictions and thus, generate an explanation. However, according to [Cynthia Rudin](https://doi.org/10.1038/s42256-019-0048-x), these explanations are unreliable as they cannot have perfect fidelity with respect to the original model. Rudin explains that if the explanation was completely faithful to what the black box model computes, the post-hoc model predictions and explanations would then be equal to the predictions of the black box model, and thus, one would not need the black box model in the first place, only the post-hoc one. But since this is not the case, this leads to the danger that any explanation method for a black box model can be an inaccurate representation of the original model. Even a post-hoc model that predicts almost identically to a black box model might use completely different features, and is thus not faithful to the computation of the black box one.\n",
        "\n",
        "As a solution, other approaches aimed to develop models that are interpretable by design; they provide their own explanations, which are faithful to what the model actually computes. For example, in image analysis, the 'this looks like that' algorithm appends a special prototype layer to the end of a deep convolutional neural network where, during training, the prototype layer finds parts of training images that act as prototypes for each class. Thus, during testing, when a new test image needs to be evaluated, the network finds parts of the test image that are similar to the prototypes it learned during training. The final class prediction of the network is based on the weighted sum of similarities to the prototypes; this is the sum of evidence throughout the image for a particular class. The explanations given by the network are the prototypes. These explanations are the actual computations of the model, and are not post-hoc explanations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk5XHKsYGi5C"
      },
      "source": [
        "In this tutorial, we will show how to use the 'this looks like that' interpretable by design algorithm to identify and classify cassava plant diseases, and provide an explanation for the predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ4k9zPvHCdE"
      },
      "source": [
        "#Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i1taaJOJshV"
      },
      "source": [
        "Before starting, you should note that graphics processing units (GPUs) can dramatically increase training speed thanks to their processing cores initially designed to process visual data such as videos and images.\n",
        "It is recommended to use a GPU instance for faster training. By default, this notebook runs on GPU. If you would like to change the instance type, check Colab docs [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV8LsirNKq6k"
      },
      "source": [
        "Let us start by checking the number and type of GPUs that Google Colab assigned us for this session:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn8uY9vKXVr4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOssi3GAK80-"
      },
      "source": [
        "The original version of the cassava dataset consists of 21,397 labeled images collected during a regular survey in Uganda where images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala. The dataset is publicly available on the Kaggle repository at https://www.kaggle.com/c/cassava-leaf-disease-classification.\n",
        "\n",
        "However, we will use a manually cleaned version of the dataset consisting of 17,190 images.\n",
        "In addition, the dataset has been split (60% training, 20% validation, 20% testing), cropped, and balanced (see Figure 4 in the paper). Please visit the following notebooks for our tutorials on:\n",
        "- [Data Splitting Using split-folders](https://colab.research.google.com/github/faridnakhle/RSG/blob/main/1.%20RSG_Data%20splitter.ipynb)\n",
        "- [Image Cropping Using the 'you only look once' (YOLO) AI Algorithm](https://colab.research.google.com/github/faridnakhle/RSG/blob/main/2.%20RSG_Leaf%20cropper.ipynb)\n",
        "- [Image Segmentation Using SegNet AI Algorithm](https://colab.research.google.com/github/faridnakhle/RSG/blob/main/3.%20RSG_Leaf%20segmenter.ipynb)\n",
        "- [Data Balancing by Oversampling with Geometric Transformations Using Augmentor](https://colab.research.google.com/github/faridnakhle/RSG/blob/main/4.%20RSG_Oversample%20with%20Augmentor.ipynb)\n",
        "- [Data Balancing by Oversampling with Synthetic Data Using Deep Convolutional Generative Adverserial Network (DCGAN) AI Algorithm](https://colab.research.google.com/github/faridnakhle/RSG/blob/main/5.%20RSG_Oversample%20with%20DCGAN.ipynb)\n",
        "- [Data Balancing by Downsampling Using K Nearest Neighbor AI Algorithm](https://colab.research.google.com/github/faridnakhle/RSG/blob/main/6.%20RSG_Downsample%20with%20KNN.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA_qOOeNQ_Qe"
      },
      "source": [
        "The following code block will download the prepared cassava dataset which is hosted on Google Drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLPayoN8aMgg"
      },
      "source": [
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P77TpjTl2v1"
      },
      "source": [
        "file_id = '13jwC684Sg1wWLhF7SjPIlsfJNuKqJ_IQ'\n",
        "destination = '/content/dataset.zip'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_dPBx6hReK-"
      },
      "source": [
        "Next, we will create a folder called 'dataset' under /content/, and extract the downloaded dataset to it. As a result, three folders should be created under /content/dataset/cdsv5/ as following:\n",
        "- train: the folder containing the training dataset.\n",
        "- train_aug: the folder containing the augmented training dataset.\n",
        "- val: the folder containing the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XkziIvnbLn8"
      },
      "source": [
        "#unzip dataset\n",
        "!mkdir /content/dataset\n",
        "!apt-get install unzip\n",
        "!unzip /content/dataset.zip -d /content/dataset/\n",
        "!rm -R  /content/dataset.zip #save some space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y96hAav4SCQS"
      },
      "source": [
        "Now that our dataset is ready, let us take a quick look on the differences between the class distribution in the original and the balanced training sets. To do so, the next code block will count all images in every class in the training and augmented datasets. A bar plot will be used to display the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsEreRBaj2My"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "train_dir = '/content/dataset/cdsv5/train/'\n",
        "train_classes = [path for path in os.listdir(train_dir)]\n",
        "train_imgs = dict([(ID, os.listdir(os.path.join(train_dir, ID))) for ID in train_classes])\n",
        "train_classes_count = []\n",
        "for trainClass in train_classes:\n",
        "  train_classes_count.append(len(train_imgs[trainClass]))\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "g = sns.barplot(x=train_classes, y=train_classes_count)\n",
        "g.set_xticklabels(labels=train_classes, rotation=30, ha='right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNXt9hnZSj3-"
      },
      "source": [
        "We can see that the training set is highly unbalanced. Let us check the distribution in the balanced folder by running the next code block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1In8p3fvcJ4w"
      },
      "source": [
        "train_dir = '/content/dataset/cdsv5/train_aug/'\n",
        "train_classes = [path for path in os.listdir(train_dir)]\n",
        "train_imgs = dict([(ID, os.listdir(os.path.join(train_dir, ID))) for ID in train_classes])\n",
        "train_classes_count = []\n",
        "for trainClass in train_classes:\n",
        "  train_classes_count.append(len(train_imgs[trainClass]))\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "g = sns.barplot(x=train_classes, y=train_classes_count)\n",
        "g.set_xticklabels(labels=train_classes, rotation=30, ha='right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3jfxeTGSsXn"
      },
      "source": [
        "We can see that the data is balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjHctr1NSjE9"
      },
      "source": [
        "Now that we have everything set, we will clone our implementation of the 'this looks like that' algorithm hosted on our [Github repository](https://github.com/HarfoucheLab/A-Primer-on-AI-in-Plant-Digital-Phenomics).\n",
        "\n",
        "It is worth mentioning that our version of the code was modified to support distributed computing and thus can run on a cluster with multiple nodes and multiple GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljFSQu3E09Bn"
      },
      "source": [
        "!git clone https://github.com/HarfoucheLab/A-Primer-on-AI-in-Plant-Digital-Phenomics.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i98dHFs_TRlZ"
      },
      "source": [
        "The code should now be located under /content/A-Primer-on-AI-in-Plant-Digital-Phenomics/py.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9185G4sTXUM"
      },
      "source": [
        "Next, we identify some settings that indicate the architecutre of our network and include other parameters, such as the dataset relevant directories (training and validation sets), the batch size, the number of workers, learning rates, etc.\n",
        "\n",
        "Feel free to change those parameters in accordance to your needs and hardware."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N43ACy-LdReg"
      },
      "source": [
        "settings = \"\"\"base_architecture = 'densenet161'\n",
        "img_size = 224\n",
        "prototype_shape = (2000, 128, 1, 1)\n",
        "num_classes = 5\n",
        "prototype_activation_function = 'log'\n",
        "add_on_layers_type = 'regular'\n",
        "\n",
        "experiment_run = '001'\n",
        "\n",
        "data_path = '/content/dataset/cdsv5/'\n",
        "train_dir = data_path + 'train_aug/'\n",
        "test_dir = data_path + 'val/'\n",
        "train_push_dir = data_path + 'train/'\n",
        "train_batch_size = 40 #80\n",
        "test_batch_size = 40\n",
        "train_push_batch_size = 64\n",
        "\n",
        "num_workers=3\n",
        "min_saving_accuracy=0.05\n",
        "\n",
        "joint_optimizer_lrs = {'features': 1e-4,\n",
        "                       'add_on_layers': 3e-3,\n",
        "                       'prototype_vectors': 3e-3}\n",
        "joint_lr_step_size = 5\n",
        "\n",
        "warm_optimizer_lrs = {'add_on_layers': 3e-3,\n",
        "                      'prototype_vectors': 3e-3}\n",
        "\n",
        "last_layer_optimizer_lr = 1e-4\n",
        "\n",
        "coefs = {\n",
        "    'crs_ent': 1,\n",
        "    'clst': 0.8,\n",
        "    'sep': -0.08,\n",
        "    'l1': 1e-4,\n",
        "}\n",
        "\n",
        "num_train_epochs = 1000\n",
        "num_warm_epochs = 5\n",
        "\n",
        "push_start = 10\n",
        "push_epochs = [i for i in range(num_train_epochs) if i % 10 == 0] \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soP1XQHQT6tC"
      },
      "source": [
        "Now that the settings are defined, we write them to a file called settings.py so that 'this looks like that' can locate and read those settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxKv0vQadnjn"
      },
      "source": [
        "text_file = open(\"/content/A-Primer-on-AI-in-Plant-Digital-Phenomics/py/settings.py\", \"w\")\n",
        "n = text_file.write(settings)\n",
        "text_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSUbmMedUEwq"
      },
      "source": [
        "Now that we are all set, we're ready to start the training process!\n",
        "As this is a data intensive step and time-consuming, we have included our pretrained model to this notebook and thus, you can skip the next code block.\n",
        "\n",
        "The parameters indicate the number of nodes and gpus which we would like to train our model on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWzEAypUeGoM"
      },
      "source": [
        "%cd /content/A-Primer-on-AI-in-Plant-Digital-Phenomics/py/\n",
        "!python3 mainDistributed.py --nodes 1 --gpus 1 --nr 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9xcTS9pVwbL"
      },
      "source": [
        "The next code block will download and extract our pretrained model hosted on Google Drive. The downloaded archive will be extracted to /content/pretrained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmKPZ7ful6zD"
      },
      "source": [
        "%cd /content/\n",
        "file_id = '12ugCaMfPdylDPPmfqzoOMWtB55k0L9tL'\n",
        "destination = '/content/pretrained.zip'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_NXhuq2mEUk"
      },
      "source": [
        "!mkdir /content/pretrained\n",
        "!unzip /content/pretrained.zip -d /content/pretrained/\n",
        "!rm -R /content/pretrained.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc2a9mkvWyt4"
      },
      "source": [
        "Now that we have our model ready, it's time to test its performance!\n",
        "The first step is to download the testing dataset from the same Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85t2okZTvVHG"
      },
      "source": [
        "#download test set:\n",
        "file_id = '1Ruy2At0G3oLlA1Gb9gz1-aMpcfJ6653B'\n",
        "destination = '/content/dataset_test.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJUesez3vm7G"
      },
      "source": [
        "!unzip /content/dataset_test.zip -d /content/dataset/cdsv5/\n",
        "!rm -R /content/dataset_test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhbfl5H5W_kI"
      },
      "source": [
        "The testing set is now located under /content/dataset/cdsv5/test/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geHAxdozXGRL"
      },
      "source": [
        "The next code block will attempt to classify every image in the test dataset, and calculate the overall accuracy of the model, along with its confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOu14zY8pWI2"
      },
      "source": [
        "%cd /content/A-Primer-on-AI-in-Plant-Digital-Phenomics/py/\n",
        "!python3 RunTestAndConfusionMatrix.py\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HbL2_DTXP7S"
      },
      "source": [
        "Let us display the normalized confusion matrix to get a better overview on the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZhLhurKxtVK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "img = mpimg.imread('/content/confusion_matrix.png')\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ViGhmfQXZIx"
      },
      "source": [
        "Now that we are satisfied with the model performance, we can generate the explanation of a specific prediction using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEZPK_G6QSZs"
      },
      "source": [
        "!python3 /content/A-Primer-on-AI-in-Plant-Digital-Phenomics/py/local_analysis.py -modeldir /content/pretrained/ -model 240_12push0.8884.pth -imgdir /content/dataset/cdsv5/test/1/ -img 931787054.jpg -imgclass 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnnWWQkAXjBg"
      },
      "source": [
        "The next code will plot the most activated region in the image, which the algorithm based its prediction on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgihU02Q2uR6"
      },
      "source": [
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/prototype_activation_map_by_top-1_prototype.png')\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HGuAcNBXsEa"
      },
      "source": [
        "Next, we display some prototypes that the activation region resembled to, and thus, we can interpret the prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGO-MWbQ65OQ"
      },
      "source": [
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/top-1_activated_prototype.png')\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/top-2_activated_prototype.png')\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/top-17_activated_prototype.png')\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpWlXM3fX5lv"
      },
      "source": [
        "Let us generate the explanation for another example!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdvYXSTO0yxx"
      },
      "source": [
        "#Explanation\n",
        "!python3 /content/A-Primer-on-AI-in-Plant-Digital-Phenomics/py/local_analysis.py -modeldir /content/pretrained/ -model 240_12push0.8884.pth -imgdir /content/dataset/cdsv5/test/1/ -img 1074333151.jpg -imgclass 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLvt5jCndtiI"
      },
      "source": [
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/prototype_activation_map_by_top-1_prototype.png')\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv_npr_idx8S"
      },
      "source": [
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/top-1_activated_prototype.png')\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/top-2_activated_prototype.png')\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = mpimg.imread('/content/dataset/cdsv5/test/1/pretrained/240_12push0.8884.pth/top-1_class_prototypes/top-17_activated_prototype.png')\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}